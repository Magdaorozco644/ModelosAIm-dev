{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efcb9f20-52d9-420f-ad71-d088f219bca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install awswrangler\n",
    "# !pip install holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9108cf1f-38d2-4e8a-acf8-64e9642c4759",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import awswrangler as wr\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "import holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5724b28b-d3c1-4762-b59e-7896abd62dad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TIMEFRAME - Only for dev purpose\n",
    "start_date = '2021-01-01'\n",
    "#end_date = '2023-12-31'\n",
    "end_date = '2024-02-03'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a409bb-76f8-40e1-a523-86ee23b19f0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DFs generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44219f77-c63a-4338-b93f-29394c1b47e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DB Setting\n",
    "bucket_name = 's3://viamericas-datalake-dev-us-east-1-283731589572-athena/'\n",
    "origin_name = 'AwsDataCatalog'\n",
    "database_name= 'analytics'\n",
    "table_name = 'daily_check_gp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b1782c2-882c-415c-b791-0093c681d41f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = wr.athena.read_sql_table(\n",
    "    table=table_name,\n",
    "    database=database_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35536510-2c7d-417c-8a0b-fdfe709f905d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Connection to daily_forex \n",
    "forex_table = 'last_daily_forex_country'\n",
    "\n",
    "rates = wr.athena.read_sql_table(\n",
    "    table=forex_table,\n",
    "    database=database_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8297124d-622a-45d7-8155-393e3fd94d2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### EFFECT OF CANCELED TRANSACTIONS ###\n",
    "# ES DISTINTA PORQUE DAILY_CHECK TIENE ALGUNOS FILTROS Y ESTA NO\n",
    "database_name= 'analytics'\n",
    "table2_name = 'daily_sales_count_cancelled_v2'## WE LOAD THE BASE WITH CANCELLATIONS\n",
    "\n",
    "df_canc = wr.athena.read_sql_table(\n",
    "    table=table2_name,\n",
    "    database=database_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a44db5-4f59-4ffe-b8fb-48659d358b0f",
   "metadata": {},
   "source": [
    "### DATA PREP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46336b11-72e3-4ba1-b73c-06c64fc6a4d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the 'date' column to datetime format\n",
    "df['day'] = pd.to_datetime(df['day'])\n",
    "# Grouping by 'payer' and 'country' concatenated for this level of granularity\n",
    "df['payer_country'] = df['payer'] + '_' + df['country']\n",
    "# Margin (when tx !=0)\n",
    "df['margin'] = df.apply(lambda row: row['gp'] / row['tx'] if row['tx'] != 0 else 0, axis=1)\n",
    "df['margin'] = df['margin'].apply(lambda x: float(x)).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03953fb8-872e-4c89-94b7-fdf20977d849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify date range\n",
    "df = df[(df['date'] >= start_date) & (df['date'] <= end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "277467ba-e794-4aaf-9739-f885d20b089b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Filtering data\n",
    "df = df[df['payer'] != 'EXPIRED ORDERS']\n",
    "df = df[df['amount'] != 0] # Excluding 0 (flag A & Flag C), defined in EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b93a864-897d-446c-b401-9e2647e32c47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df.isna().sum() # Reviso si hay valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66759f8c-e8d1-41f9-89ab-1fc95c7f4679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FOREX - Selecting columns & renaming\n",
    "rates['day'] = pd.to_datetime(rates['day'])\n",
    "rates = rates[['day','country','max_feed_price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac5ce35c-de89-4519-8e7a-54bfc77363e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CANCELLATIONS \n",
    "df_canc['date'] = pd.to_datetime(df_canc['date'])\n",
    "df_canc['payer_country'] = df_canc['payer'] +'_'+ df_canc['country']\n",
    "# Specific date range\n",
    "df_canc = df_canc[(df_canc['date'] >= start_date) & (df_canc['date'] <= end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e878103e-04a1-4f9a-b935-3522440d84f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fill_missing_dates(df, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fill missing dates in the DataFrame with zero values and ensure all date ranges are covered.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input DataFrame with columns 'date', 'amount', 'tx_cancelled', 'payer_country', etc.\n",
    "        start_date (str or datetime.date): Start date of the desired date range.\n",
    "        end_date (str or datetime.date): End date of the desired date range.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with missing dates filled and all date ranges covered.\n",
    "    \"\"\"\n",
    "    # Convert the 'date' column to datetime type if it's not already\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Define the desired date range\n",
    "    date_range = pd.date_range(start=start_date, end=end_date)\n",
    "    \n",
    "    # Get the minimum and maximum date range for each 'payer_country', 'id_country', 'id_main_branch'\n",
    "    \n",
    "    # Group by 'payer_country', 'id_country', 'id_main_branch' and aggregate min and max dates\n",
    "    payer_country_ranges = df.groupby(['payer_country','id_country','id_main_branch'])['date'].agg(['min', 'max']).reset_index()\n",
    "    \n",
    "    # Fill missing 'min' and 'max' dates with start_date and end_date respectively\n",
    "    payer_country_ranges['min'] = payer_country_ranges['min'].fillna(pd.to_datetime(start_date))\n",
    "    payer_country_ranges['max'] = payer_country_ranges['max'].fillna(pd.to_datetime(end_date))\n",
    "    \n",
    "    # Combine the original DataFrame with the DataFrame of all date combinations\n",
    "    df_filled = pd.DataFrame()\n",
    "    for index, row in payer_country_ranges.iterrows():\n",
    "        payer_country = row['payer_country']\n",
    "        start_payer = row['min']\n",
    "        end_payer = row['max']\n",
    "        payer_id_country = row['id_country']\n",
    "        payer_id_main_branch = row['id_main_branch']\n",
    "        \n",
    "        # Filter the original DataFrame by 'payer_country'\n",
    "        df_payer = df[df['payer_country'] == payer_country]\n",
    "        \n",
    "        # Fill missing values in 'payer_country' date range\n",
    "        date_range_payer = pd.date_range(start=start_payer, end=end_payer)\n",
    "        date_combinations = pd.DataFrame({'date': date_range_payer, 'payer_country': payer_country, 'id_country': payer_id_country, 'id_main_branch':payer_id_main_branch})\n",
    "        df_combined = pd.merge(date_combinations, df_payer, on=['date', 'payer_country'], how='left')\n",
    "        \n",
    "        # Fill missing numeric values with zero\n",
    "        numeric_columns = ['amount', 'coupon_count', 'tx', 'gp', 'margin']\n",
    "        df_combined[numeric_columns] = df_combined[numeric_columns].fillna(0)\n",
    "        \n",
    "        # Fill missing 'payer' and 'country' values using the ffill method\n",
    "        df_combined[['payer', 'country','id_country','id_main_branch']] = df_combined[['payer', 'country','id_country_x','id_main_branch_x']].ffill()\n",
    "        \n",
    "        # Fill missing 'day' values with 'date' values when NaN\n",
    "        df_combined['day'] = df_combined['day'].fillna(df_combined['date'])\n",
    "        \n",
    "        df_filled = pd.concat([df_filled, df_combined], ignore_index=True)\n",
    "    \n",
    "    # Remove redundant columns\n",
    "    df_filled = df_filled.drop(columns=['id_country_x','id_country_y','id_main_branch_x','id_main_branch_y'])\n",
    "    \n",
    "    return df_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "310b7e06-2580-4791-9def-e9ab6d33d22b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fill missing dates in df_filtered\n",
    "df_filled = fill_missing_dates(df, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357a924a-5267-4679-9e45-44d2b7ef1197",
   "metadata": {
    "tags": []
   },
   "source": [
    "### UNIVERSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3650fdb8-4062-4f75-8a4c-6110f10de6f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# AGING FILTER\n",
    "\n",
    "def aging_filter(df):\n",
    "    \"\"\"\n",
    "    Filter a DataFrame based on aging criteria described in aging.ipynb\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input DataFrame with columns 'date', 'payer_country', 'amount', and 'tx'.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Filtered DataFrame containing only the rows that meet the aging criteria.\n",
    "    \"\"\"\n",
    "    # Find the last date in the sample\n",
    "    last_date_sample = df['day'].max()\n",
    "\n",
    "    # Calculate the limit date, one day before the last date in the sample\n",
    "    limit_date = last_date_sample - pd.Timedelta(days=1)\n",
    "\n",
    "    # Aggregate data by 'payer_country'\n",
    "    result = (\n",
    "        df.groupby('payer_country')\n",
    "        .agg(\n",
    "            first_date=('day', 'min'),\n",
    "            last_date=('day', 'max'),\n",
    "            total_amount=('amount', 'sum'),\n",
    "            total_transactions=('tx', 'sum')\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Calculate age of payer\n",
    "    result['age_payer'] = ((limit_date - result['first_date']).dt.days / 30).round(2)\n",
    "\n",
    "    # Calculate active time\n",
    "    result['active_time'] = ((result['last_date'] - result['first_date']).dt.days / 30).round(2)\n",
    "\n",
    "    # Calculate inactive time\n",
    "    result['inactive_time'] = ((limit_date - result['last_date']).dt.days / 30).round(2)\n",
    "\n",
    "    # Sort the DataFrame by 'total_amount' from highest to lowest\n",
    "    result = result.sort_values(by='total_amount', ascending=False)\n",
    "\n",
    "    # Filter the DataFrame based on conditions\n",
    "    aging_universe = result.loc[\n",
    "        (result.age_payer >= 3) & \n",
    "        (result.inactive_time <= 3) & \n",
    "        (result.total_amount > 10000) & \n",
    "        (result.total_transactions > 50)\n",
    "    ]\n",
    "    \n",
    "    return aging_universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1970265-205a-4205-a298-6d55ebde490b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_173/2252516470.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['day'] = pd.to_datetime(df_filtered['day'])\n"
     ]
    }
   ],
   "source": [
    "# Defining Universe\n",
    "df_aging = aging_filter(df_filled) #Filtering 'payer_country' based on Aging notebook\n",
    "df_filtered = df_filled[df_filled['payer_country'].isin(df_aging['payer_country'])] # Applying aging filters \n",
    "df_filtered['day'] = pd.to_datetime(df_filtered['day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0631e931-d705-48f3-8a5d-7d47edca6e91",
   "metadata": {
    "tags": []
   },
   "source": [
    "### VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ac793bd-cc12-41ad-9b33-f60a0a294041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_lag_and_variation(df, num_lags):\n",
    "    \"\"\"\n",
    "    Generate lagged values and variations for a given df\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input df with columns 'symbol' and 'feed_price'.\n",
    "        num_lags (int): Number of lagged values to generate.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: df with lagged values and variations added as new columns.\n",
    "    \"\"\"\n",
    "    # Create columns for each day's lag up to the defined maximum\n",
    "    for i in range(1, num_lags + 1):\n",
    "        col_name = f'rate_lag_{i}'\n",
    "        # Shift the 'feed_price' column grouped by 'symbol'\n",
    "        df[col_name] = df.groupby('country')['max_feed_price'].shift(i)\n",
    "\n",
    "    # Calculate the variation columns between consecutive lags\n",
    "    for i in range(1, num_lags):\n",
    "        col_name = f'var_rate_lag_{i}'\n",
    "        # Calculate the difference between consecutive lag columns\n",
    "        df[col_name] = df[f'rate_lag_{i}'] - df[f'rate_lag_{i + 1}']\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1076de06-6aa9-4e6f-a876-5bf1f5cd05e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rates_number = 30\n",
    "rates = rates.sort_values(['country','day'])\n",
    "rates = generate_lag_and_variation(rates, rates_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69c54aeb-1d25-44c7-ae3a-02da228415f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First step: Rates +  df_filtered (defined by Universe)\n",
    "df1 = pd.merge(df_filtered, rates, on=['day', 'country'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eac77082-dc5d-437e-853c-3a36acef2a63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1['date'] = pd.to_datetime(df1['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86ee53d4-26bc-413f-884b-235a94912e43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fill_missing_dates(df, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fill missing dates in the DataFrame with zero values and ensure all date ranges are covered.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input DataFrame with columns 'date', 'amount', 'tx_cancelled', 'payer_country', etc.\n",
    "        start_date (str or datetime.date): Start date of the desired date range.\n",
    "        end_date (str or datetime.date): End date of the desired date range.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with missing dates filled and all date ranges covered.\n",
    "    \"\"\"\n",
    "    # Create an empty DataFrame with the specified date range\n",
    "    date_range = pd.date_range(start=start_date, end=end_date)\n",
    "    df_fill = pd.DataFrame({'date': date_range, 'amount': 0, 'tx_cancelled': 0})\n",
    "    df_fill['date'] = pd.to_datetime(df_fill['date']).dt.date\n",
    "\n",
    "    # Sort the original DataFrame by 'country', 'payer', and 'date'\n",
    "    df = df.sort_values(by=['country', 'payer', 'date'])\n",
    "\n",
    "    # Create an empty DataFrame to hold the result\n",
    "    result_df = pd.DataFrame()\n",
    "\n",
    "    # Loop through each 'payer_country'\n",
    "    for payer_country in df['payer_country'].unique():\n",
    "        # Filter DataFrame by 'payer_country'\n",
    "        df_aux = df[df['payer_country'] == payer_country]\n",
    "\n",
    "        # Combine df_aux (payer_country) with df_fill, keeping values from df_aux and filling missing dates\n",
    "        merged_df = df_aux.set_index('date').combine_first(df_fill.set_index('date')).reset_index()\n",
    "\n",
    "        # Fill missing values in specified columns\n",
    "        columns_to_fill = ['payer', 'country', 'payer_country']\n",
    "        merged_df[columns_to_fill] = merged_df[columns_to_fill].ffill().bfill()\n",
    "\n",
    "        # Concatenate the result with the final DataFrame\n",
    "        result_df = pd.concat([result_df, merged_df], ignore_index=True)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d58f29d3-3970-46ab-9acc-d2174a957a16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call the function with the specified start_date and end_date\n",
    "df_full = fill_missing_dates(df_canc, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bdaa0c37-2e8c-4177-8e19-b67be69771dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_tx_lags_and_variation(df, tx_count):\n",
    "    \"\"\"\n",
    "    Generate lag columns for cancelled transactions and their variations.\n",
    "\n",
    "    Args:\n",
    "    - df: DataFrame containing transaction data\n",
    "    - tx_count: Number of periods for lag calculation\n",
    "\n",
    "    Returns:\n",
    "    - df: DataFrame with added lag and variation columns\n",
    "    \"\"\"\n",
    "    # Sort the dataset based on country, payer, and date\n",
    "    df = df.sort_values(by=['country', 'payer', 'date'])\n",
    "\n",
    "    # Create columns for each day's lag up to the defined maximum\n",
    "    for i in range(1, tx_count + 1):\n",
    "        col_name = f'tx_cancelled_lag_{i}'\n",
    "        # Shift the 'tx_cancelled' column grouped by 'country' and 'payer'\n",
    "        df[col_name] = df.groupby(['country', 'payer'])['tx_cancelled'].shift(i)\n",
    "\n",
    "    # Calculate the variation columns between consecutive delays\n",
    "    for i in range(1, tx_count):\n",
    "        col_name = f'var_tx_cancelled_lag_{i}'\n",
    "        # Calculate the difference between consecutive lag columns\n",
    "        df[col_name] = df[f'tx_cancelled_lag_{i}'] - df[f'tx_cancelled_lag_{i + 1}']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "389600d9-6cc1-453c-8c56-40cadc70211d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call the function and assign the result back to df2\n",
    "tx_cancelled_lags = 30\n",
    "df2 = generate_tx_lags_and_variation(df_full, tx_cancelled_lags)\n",
    "df2['day'] = pd.to_datetime(df2['day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "42394f6c-ab41-4e40-a1e4-06f8235843a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Coupon ratio\n",
    "df1['ratio_coupon_tx']=df1.coupon_count/df1.tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5988f633-513f-4761-bb43-9097dde4ebf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_coupon_tx_lags(df, tx_count):\n",
    "    \"\"\"\n",
    "    Generate lag columns for coupon_tx ratio\n",
    "\n",
    "    Args:\n",
    "    - df: DataFrame containing transaction data\n",
    "    - tx_count: Number of periods for lag calculation\n",
    "\n",
    "    Returns:\n",
    "    - df: DataFrame with added lag and variation columns\n",
    "    \"\"\"\n",
    "    # Sort the dataset based on country, payer, and date\n",
    "    df = df.sort_values(by=['country', 'payer', 'date'])\n",
    "\n",
    "    # Create columns for each day's lag up to the defined maximum\n",
    "    for i in range(1, tx_count + 1):\n",
    "        col_name = f'ratio_coupon_tx_lag_{i}'\n",
    "        # Shift the 'ratio_coupon_tx' column grouped by 'country' and 'payer'\n",
    "        df[col_name] = df.groupby(['country', 'payer'])['ratio_coupon_tx'].shift(i)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ac53929d-bac1-4b24-9304-8e59be0c895f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call the function and assign the result back to df1\n",
    "tx_ratio_coupon_tx_lags = 30\n",
    "df1 = generate_coupon_tx_lags(df1, tx_ratio_coupon_tx_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5abe8b0f-9c61-4a65-9a6e-a5db1dc689da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_tx_lags(df, tx_count):\n",
    "    \"\"\"\n",
    "    Generate lags columns for txs\n",
    "\n",
    "    Args:\n",
    "    - df: DataFrame containing transaction data\n",
    "    - tx_count: Number of periods for lag calculation\n",
    "\n",
    "    Returns:\n",
    "    - df: DataFrame with added lag and variation columns\n",
    "    \"\"\"\n",
    "    # Sort the dataset based on country, payer, and date\n",
    "    df = df.sort_values(by=['country', 'payer', 'date'])\n",
    "\n",
    "    # Create columns for each day's lag up to the defined maximum\n",
    "    for i in range(1, tx_count + 1):\n",
    "        col_name = f'tx_lag_{i}'\n",
    "        # Shift the 'tx' column grouped by 'country' and 'payer'\n",
    "        df[col_name] = df.groupby(['country', 'payer'])['tx'].shift(i)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dd68de25-0b1c-4bdb-81e9-80c4ad1eb971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call the function and assign the result back to df1\n",
    "tx_lags = 30\n",
    "df1 = generate_tx_lags(df1, tx_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9616bf84-c56c-4f84-b992-6157cf0ba8de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_margin_lags(df, margin_lags):\n",
    "    \"\"\"\n",
    "    Generate lag columns for margin\n",
    "\n",
    "    Args:\n",
    "    - df: DataFrame containing transaction data\n",
    "    - margin_lags: Number of periods for lag calculation\n",
    "\n",
    "    Returns:\n",
    "    - df: DataFrame with added lag columns for margin\n",
    "    \"\"\"\n",
    "    # Sort the dataset based on country, payer, and date\n",
    "    df = df.sort_values(by=['country', 'payer', 'date'])\n",
    "\n",
    "    # Create columns for each day's lag up to the defined maximum\n",
    "    for i in range(1, margin_lags + 1):\n",
    "        col_name = f'margin_lag_{i}'\n",
    "        # Shift the 'margin' column grouped by 'country' and 'payer'\n",
    "        df[col_name] = df.groupby(['country', 'payer'])['margin'].shift(i)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0f2324e4-a8f8-401b-89f7-7c335753775e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call the function and assign the result back to df1\n",
    "margin_lags = 10\n",
    "df1 = generate_margin_lags(df1, margin_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f3cce428-798b-4914-b5b1-c5cda9d08a9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merging dataframes\n",
    "df_final = pd.merge(df1, df2, on=['date', 'payer', 'country', 'payer_country', 'amount'], how='inner')\n",
    "df_final['date'] = pd.to_datetime(df_final['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dafc713-924a-453a-b70b-4379b55e716e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DUMMIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "49ef881a-3adc-4a51-b553-695b46f48a82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mark_us_holidays(df):\n",
    "    \"\"\"\n",
    "    Mark US holidays, excluding specified holidays and those with 'Observed'.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame containing a 'date' column in datetime format.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with an additional 'is_holiday' column, where 1 indicates a US holiday and 0 otherwise.\n",
    "    \"\"\"\n",
    "    # Get the minimum and maximum dates\n",
    "    min_date = df['date'].min().year\n",
    "    max_date = df['date'].max().year + 1\n",
    "    print(min_date, max_date)\n",
    "    \n",
    "    # Load US holidays\n",
    "    us_holidays = holidays.US(years=range(min_date, max_date))\n",
    "\n",
    "    # List of holidays to exclude\n",
    "    holidays_to_exclude = [\"Washington's Birthday\", \"Columbus Day\"]\n",
    "    \n",
    "    # Filter holidays that should be excludeds\n",
    "    filtered_holidays = {date: name for date, name in us_holidays.items() if name not in holidays_to_exclude and 'observed' not in name.lower() }\n",
    "#    print(filtered_holidays) # Habilitando este print puedo ver que feriados son los que estamos marcando\n",
    "    \n",
    "    # Create a list of holiday dates\n",
    "    holidays_list = list(filtered_holidays.keys())\n",
    "    \n",
    "    # Mark holidays in the DataFrame\n",
    "    df['is_holiday'] = df['date'].isin(holidays_list).astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7594a751-1914-40ed-a430-7a092ecb9b41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021 2025\n"
     ]
    }
   ],
   "source": [
    "#Applying holiday function \n",
    "df_final = mark_us_holidays(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "799b709e-83ef-4080-98ff-5830d7fde1a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mark_fourth_july(df):\n",
    "    \"\"\"\n",
    "    Mark the Fourth of July in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame containing a 'date' column in datetime format.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with an additional 'is_fourth_of_july' column.\n",
    "    \"\"\"\n",
    "    # Check if the date is the Fourth of July\n",
    "    df['is_fourth_of_july'] = (\n",
    "        (df['date'].dt.month == 7) & (df['date'].dt.day == 4)\n",
    "    ).astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "54d143c9-f34e-4e50-b129-c8b2c20243e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_final = mark_fourth_july(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dc0d0b51-775e-4b85-ac3b-8b6a0dfe0f66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_var_30ds(window, row, df_final):\n",
    "    \"\"\"\n",
    "    Calculate the variable 'var_30ds' based on the average amount in the last 30 days.\n",
    "\n",
    "    Parameters:\n",
    "    window (int): The window size in days for the calculation.\n",
    "    row (pandas.Series): The row containing the data for the current observation.\n",
    "    df_final (pandas.DataFrame): The DataFrame containing the final dataset.\n",
    "\n",
    "    Returns:\n",
    "    float or None: The calculated variable 'var_30ds' if applicable, else None.\n",
    "    \"\"\"\n",
    "    # Check if the current day is a holiday\n",
    "    if row['is_holiday'] == 1:\n",
    "        # Filter the DataFrame to get only the last 30 days for the current 'payer_country'\n",
    "        filter_condition = (df_final['payer_country'] == row['payer_country']) & \\\n",
    "                           (df_final['date'] >= (row['date'] - pd.Timedelta(days=window))) & \\\n",
    "                           (df_final['date'] < row['date'])\n",
    "        filtered_df = df_final[filter_condition]\n",
    "        \n",
    "        # Calculate the average amount for the current 'payer_country' in the last 30 days\n",
    "        avg_amount = filtered_df['amount'].mean()\n",
    "\n",
    "        # Print filtered DataFrame for debugging\n",
    "#        if (row['payer_country'] == 'ELEKTRA (MEXICO)_MEXICO') and (row['date'] == datetime.strptime('2023-09-04', '%Y-%m-%d')):\n",
    "#            print(filtered_df)\n",
    "        \n",
    "        # Calculate var_30ds according to the specified formula\n",
    "        if avg_amount != 0 and row['amount'] != 0:\n",
    "            var_30ds = float(row['amount']) / float(avg_amount) - 1  # Convert avg_amount to float before division\n",
    "            return var_30ds  \n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "660e804e-2eb0-48b0-9d3e-1e1b92d42847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Applying calculate_var_30ds to each holiday date\n",
    "window = 30\n",
    "df_final['var_30ds'] = df_final.apply(lambda row: calculate_var_30ds(window, row, df_final), axis=1)\n",
    "df_final['var_30ds'] = df_final['var_30ds'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a4dedfd0-a5d8-4d43-85c5-03c3055e2945",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mark_christmas_day(df):\n",
    "    \"\"\"\n",
    "    Marks Christmas Day (December 25th) in the DataFrame.\n",
    "\n",
    "    This function identifies December 25th for each year present in the DataFrame\n",
    "    and marks it as Christmas Day in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The DataFrame containing the date column.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with Christmas Day marked.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If the DataFrame does not contain a 'date' column.\n",
    "    \"\"\"\n",
    "    # Check if the 'date' column exists in the DataFrame\n",
    "    if 'date' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain a 'date' column.\")\n",
    "\n",
    "    # Create a new column to mark Christmas Day\n",
    "    df['christmas_day'] = 0\n",
    "\n",
    "    # Iterate over each year present in the DataFrame\n",
    "    for year in df['date'].dt.year.unique():\n",
    "        # Mark December 25th for the current year\n",
    "        christmas_date = datetime(year, 12, 25)\n",
    "        # Mark rows corresponding to Christmas Day for the current year\n",
    "        df.loc[(df['date'].dt.year == year) & (df['date'].dt.month == 12) & (df['date'].dt.day == 25), 'christmas_day'] = 1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "39b6dcce-2032-47e6-9efb-6f133ec21141",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_final = mark_christmas_day(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "74921e23-967c-4a4b-a976-6d84a9f941e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mark_new_year_day(df):\n",
    "    \"\"\"\n",
    "    Marks New year (January 1st) in the DataFrame.\n",
    "\n",
    "    This function identifies January 1st for each year present in the DataFrame\n",
    "    and marks it as Christmas Day in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The DataFrame containing the date column.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with New Year marked.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If the DataFrame does not contain a 'date' column.\n",
    "    \"\"\"\n",
    "    # Check if the 'date' column exists in the DataFrame\n",
    "    if 'date' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain a 'date' column.\")\n",
    "\n",
    "    # Create a new column to mark New Year's Day\n",
    "    df['new_year_day'] = 0\n",
    "\n",
    "    # Iterate over each year present in the DataFrame\n",
    "    for year in df['date'].dt.year.unique():\n",
    "        # Mark January 1st for the current year\n",
    "        new_year_date = datetime(year, 1, 1)\n",
    "        # Mark rows corresponding to New Year's Day for the current year\n",
    "        df.loc[(df['date'].dt.year == year) & (df['date'].dt.month == 1) & (df['date'].dt.day == 1), 'new_year_day'] = 1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d6c10019-ddce-4278-ada2-8355293ebd30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_final = mark_new_year_day(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b8120a70-51d1-497f-a70d-e27a18219601",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mark_post_holiday(df):\n",
    "    \"\"\"\n",
    "    Mark days after holidays. Usually post holiday days tend to rise sales\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame containing a 'is_holiday' column indicating holidays.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with an additional 'post_holiday' column, where 1 indicates a day after a holiday.\n",
    "    \"\"\"\n",
    "    post_holiday = []\n",
    "    for idx, row in df.iterrows():\n",
    "        is_holiday = row['is_holiday']\n",
    "        if is_holiday == 1:\n",
    "            post_holiday.append(0)\n",
    "        else:\n",
    "            if idx > 0 and df.loc[idx - 1, 'is_holiday'] == 1:\n",
    "                post_holiday.append(1)\n",
    "            else:\n",
    "                post_holiday.append(0)\n",
    "    df['post_holiday'] = post_holiday\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5cb8fe11-0d1c-4641-b2b8-7e921355027a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_final = mark_post_holiday(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b5c54547-dba8-48a5-aa3a-e135224d7c32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def correcting_holidays(df, holidays_to_exclude):\n",
    "    \"\"\"\n",
    "    Marks specified dates in the list as non-holidays.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame containing an 'is_holiday' column indicating holidays.\n",
    "        holidays_to_exclude (list): List of dates in month-year format to be marked as non-holidays.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Modified DataFrame with dates marked as non-holidays in the 'is_holiday' column.\n",
    "    \"\"\"\n",
    "    # Convertir las fechas a formato mes-día (mm-dd) para comparación\n",
    "    df['month_day'] = df['date'].dt.strftime('%m-%d')\n",
    "\n",
    "    # Marcar como no festivo (0) los días que están en la lista de fechas a excluir\n",
    "    df.loc[df['month_day'].isin(holidays_to_exclude), 'is_holiday'] = 0\n",
    "\n",
    "    # Eliminar la columna temporal 'month_day'\n",
    "    df.drop(columns=['month_day'], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3fcc3e26-4f53-499f-b321-22bee8e3b89f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "holidays_to_exclude = ['07-04', '12-25', '01-01']  # Formato mes-día\n",
    "\n",
    "# Applying fuction\n",
    "df_final = correcting_holidays(df_final, holidays_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e12e1ced-8b57-40da-a17a-f33fcf7f98b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_173/1811074113.py:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0' has dtype incompatible with datetime64[ns], please explicitly cast to a compatible dtype first.\n",
      "  df_final.fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Filling NaN in exogenous and lags\n",
    "df_final.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc95039-4242-460e-8366-1956c7a21c91",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Saving df_final to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0b0537d4-0277-4c56-90f0-9ae1f840ae2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paths': ['s3://viamericas-datalake-dev-us-east-1-283731589572-analytics/ABTv3_update/ABTv3_update.csv'],\n",
       " 'partitions_values': {}}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# S3 Settings\n",
    "\n",
    "bucket = 'viamericas-datalake-dev-us-east-1-283731589572-analytics'\n",
    "prefix_abt = 'ABTv3_update'\n",
    "file_name = 'ABTv3_update.csv'\n",
    "\n",
    "# S3 path\n",
    "s3_path = f\"s3://{bucket}/{prefix_abt}/{file_name}\"\n",
    "\n",
    "#Saving\n",
    "wr.s3.to_csv(df_final, path=s3_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
